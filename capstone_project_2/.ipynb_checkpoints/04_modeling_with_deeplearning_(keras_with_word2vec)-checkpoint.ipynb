{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Keras with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    12080\n",
      "0     1192\n",
      "Name: rating, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great Hoses Good USA company that stands behin...</td>\n",
       "      <td>great hose good usa company stand behind produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gilmour 10-58050 8-ply Flexogen Hose 5/8-Inch ...</td>\n",
       "      <td>gilmour ply flexogen hose inch foot green high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Very satisfied! It's probably one of the best ...</td>\n",
       "      <td>satisfied probably one best hose ever pro good...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                        review_text  \\\n",
       "0       1  Great Hoses Good USA company that stands behin...   \n",
       "1       1  Gilmour 10-58050 8-ply Flexogen Hose 5/8-Inch ...   \n",
       "2       1  Very satisfied! It's probably one of the best ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  great hose good usa company stand behind produ...  \n",
       "1  gilmour ply flexogen hose inch foot green high...  \n",
       "2  satisfied probably one best hose ever pro good...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('data_capstone_2/nlp_reviews_cleaned.csv', delimiter=',', encoding='utf-8')\n",
    "\n",
    "df['rating'] = df['rating'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "print(df.rating.value_counts())\n",
    "\n",
    "df = df.drop(['Unnamed: 0', 'customer', 'product', 'time', 'pos_feedback',\n",
    "       'neg_feedback'], axis=1)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "review_lines = list()\n",
    "lines = df['review_text'].values.tolist()\n",
    "\n",
    "for line in lines:   \n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13272"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 44864\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "# vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "filename = 'patio_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vegetable', 0.8768568634986877),\n",
       " ('bed', 0.8264282941818237),\n",
       " ('treasures', 0.8245724439620972),\n",
       " ('depotwell', 0.8244673013687134),\n",
       " ('gardens', 0.8243735432624817),\n",
       " ('beds', 0.8168197870254517),\n",
       " ('cultivate', 0.8070510625839233),\n",
       " ('potted', 0.8069136142730713),\n",
       " ('diedyou', 0.8049212694168091),\n",
       " ('flower', 0.7979527711868286)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us try some utility functions of gensim word2vec more details here \n",
    "\n",
    "model.wv.most_similar('garden')#, topn =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lawnthis', 1.3675940036773682),\n",
       " ('trees', 1.3167675733566284),\n",
       " ('wateringnow', 1.313320279121399),\n",
       " ('agave', 1.31088387966156),\n",
       " ('gardens', 1.2859127521514893),\n",
       " ('providedi', 1.2753938436508179),\n",
       " ('divots', 1.265055537223816),\n",
       " ('bushes', 1.2624788284301758),\n",
       " ('shrubs', 1.2610939741134644),\n",
       " ('weeds', 1.2534018754959106)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s see the result of semantically reasonable word vectors (king - man + woman)\n",
    "model.wv.most_similar_cosmul(positive=['garden', 'lawn'], negative=['hose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gardens', 0.783897340297699),\n",
       " ('bushes', 0.7665226459503174),\n",
       " ('shrubs', 0.7641263008117676),\n",
       " ('trees', 0.763062596321106),\n",
       " ('lawnthis', 0.7563513517379761),\n",
       " ('weeds', 0.7559521198272705),\n",
       " ('agave', 0.7450824975967407),\n",
       " ('downed', 0.724432110786438),\n",
       " ('backyard', 0.7189232110977173),\n",
       " ('growing', 0.7179781198501587)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s see the result of semantically reasonable word vectors (king - man + woman)\n",
    "model.wv.most_similar(positive=['garden', 'lawn'], negative=['hose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hose\n"
     ]
    }
   ],
   "source": [
    "#odd word out\n",
    "print(model.wv.doesnt_match(\"garden king lawn hose\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vegetable', 0.8768568634986877),\n",
       " ('bed', 0.8264282941818237),\n",
       " ('treasures', 0.8245724439620972),\n",
       " ('depotwell', 0.8244673013687134),\n",
       " ('gardens', 0.8243735432624817),\n",
       " ('beds', 0.8168197870254517),\n",
       " ('cultivate', 0.8070510625839233),\n",
       " ('potted', 0.8069136142730713),\n",
       " ('diedyou', 0.8049212694168091),\n",
       " ('flower', 0.7979527711868286)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"garden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5844543\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('garden', 'lawn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'patio_embedding_word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:10000, 'review_text'].values\n",
    "y_train = df.loc[:10000, 'rating'].values\n",
    "X_test = df.loc[10001:, 'review_text'].values\n",
    "y_test = df.loc[10001:, 'rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = df.review_text.values\n",
    "max_length = max([len(s.split()) for s in total_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44864 unique tokens.\n",
      "Shape of review tensor: (13272, 2263)\n",
      "Shape of sentiment tensor: (13272,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(review_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment =  df['rating'].values\n",
    "print('Shape of review tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor:', sentiment.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad tensor: (10618, 2263)\n",
      "Shape of y_train tensor: (10618,)\n",
      "Shape of X_test_pad tensor: (2654, 2263)\n",
      "Shape of y_test tensor: (2654,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad tensor:', X_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44865\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2263, 100)         4486500   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2259, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1129, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 144512)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 144513    \n",
      "=================================================================\n",
      "Total params: 4,695,141\n",
      "Trainable params: 208,641\n",
      "Non-trainable params: 4,486,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10618 samples, validate on 2654 samples\n",
      "Epoch 1/25\n",
      " - 241s - loss: 0.3013 - acc: 0.9058 - val_loss: 0.2555 - val_acc: 0.9145\n",
      "Epoch 2/25\n",
      " - 228s - loss: 0.2513 - acc: 0.9099 - val_loss: 0.2527 - val_acc: 0.9141\n",
      "Epoch 3/25\n",
      " - 4821s - loss: 0.2205 - acc: 0.9163 - val_loss: 0.2621 - val_acc: 0.9062\n",
      "Epoch 4/25\n",
      " - 275s - loss: 0.1823 - acc: 0.9287 - val_loss: 0.2641 - val_acc: 0.9107\n",
      "Epoch 5/25\n",
      " - 288s - loss: 0.1497 - acc: 0.9406 - val_loss: 0.2738 - val_acc: 0.9141\n",
      "Epoch 6/25\n",
      " - 292s - loss: 0.1129 - acc: 0.9585 - val_loss: 0.3124 - val_acc: 0.9118\n",
      "Epoch 7/25\n",
      " - 321s - loss: 0.0832 - acc: 0.9717 - val_loss: 0.3164 - val_acc: 0.8900\n",
      "Epoch 8/25\n",
      " - 343s - loss: 0.0618 - acc: 0.9824 - val_loss: 0.3429 - val_acc: 0.8979\n",
      "Epoch 9/25\n",
      " - 7514s - loss: 0.0451 - acc: 0.9888 - val_loss: 0.3710 - val_acc: 0.9009\n",
      "Epoch 10/25\n",
      " - 271s - loss: 0.0322 - acc: 0.9940 - val_loss: 0.3964 - val_acc: 0.9002\n",
      "Epoch 11/25\n",
      " - 284s - loss: 0.0229 - acc: 0.9968 - val_loss: 0.4046 - val_acc: 0.8881\n",
      "Epoch 12/25\n",
      " - 290s - loss: 0.0170 - acc: 0.9986 - val_loss: 0.4408 - val_acc: 0.8968\n",
      "Epoch 13/25\n",
      " - 302s - loss: 0.0131 - acc: 0.9991 - val_loss: 0.4714 - val_acc: 0.9009\n",
      "Epoch 14/25\n",
      " - 304s - loss: 0.0097 - acc: 0.9995 - val_loss: 0.5005 - val_acc: 0.9050\n",
      "Epoch 15/25\n",
      " - 3328s - loss: 0.0078 - acc: 0.9997 - val_loss: 0.4904 - val_acc: 0.8941\n",
      "Epoch 16/25\n",
      " - 1308s - loss: 0.0064 - acc: 0.9996 - val_loss: 0.5286 - val_acc: 0.9002\n",
      "Epoch 17/25\n",
      " - 252s - loss: 0.0051 - acc: 0.9997 - val_loss: 0.5418 - val_acc: 0.8998\n",
      "Epoch 18/25\n",
      " - 280s - loss: 0.0042 - acc: 0.9998 - val_loss: 0.5397 - val_acc: 0.8960\n",
      "Epoch 19/25\n",
      " - 253s - loss: 0.0037 - acc: 0.9998 - val_loss: 0.5639 - val_acc: 0.8998\n",
      "Epoch 20/25\n",
      " - 244s - loss: 0.0032 - acc: 0.9998 - val_loss: 0.5640 - val_acc: 0.8960\n",
      "Epoch 21/25\n",
      " - 237s - loss: 0.0028 - acc: 0.9998 - val_loss: 0.5899 - val_acc: 0.9005\n",
      "Epoch 22/25\n",
      " - 2405s - loss: 0.0025 - acc: 0.9998 - val_loss: 0.5956 - val_acc: 0.8990\n",
      "Epoch 23/25\n",
      " - 290s - loss: 0.0022 - acc: 0.9998 - val_loss: 0.6047 - val_acc: 0.8998\n",
      "Epoch 24/25\n",
      " - 287s - loss: 0.0019 - acc: 0.9998 - val_loss: 0.6137 - val_acc: 0.8990\n",
      "Epoch 25/25\n",
      " - 252s - loss: 0.0017 - acc: 0.9999 - val_loss: 0.6210 - val_acc: 0.8983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a256d67f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2654/2654 [==============================] - 29s 11ms/step\n",
      "Accuracy: 89.826677\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97867787],\n",
       "       [0.99898213],\n",
       "       [0.40239662],\n",
       "       [0.9764854 ],\n",
       "       [0.7732555 ],\n",
       "       [0.79503137],\n",
       "       [0.99898213],\n",
       "       [0.15101512]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us test some  samples\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "\n",
    "test_sample_1 = \"This garden is beautiful!\"\n",
    "test_sample_2 = \"Good hose!\"\n",
    "test_sample_3 = \"Maybe I like this lawnmover.\"\n",
    "test_sample_4 = \"Not to my taste, will skip and use another hose\"\n",
    "test_sample_5 = \"if you like fruits, then this product might be good for you.\"\n",
    "test_sample_6 = \"Bad rubber!\"\n",
    "test_sample_7 = \"Not a good vegetable!\"\n",
    "test_sample_8 = \"This hose really sucks! Can I get my money back please?\"\n",
    "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]\n",
    "\n",
    "test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
    "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=2263)\n",
    "\n",
    "#predict\n",
    "model.predict(x=test_samples_tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the built model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 2263, 100)         4486500   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,499,301\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 4,486,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 10618 samples, validate on 2654 samples\n",
      "Epoch 1/25\n",
      " - 379s - loss: 0.3215 - acc: 0.9053 - val_loss: 0.2845 - val_acc: 0.9145\n",
      "Epoch 2/25\n",
      " - 365s - loss: 0.2904 - acc: 0.9090 - val_loss: 0.2739 - val_acc: 0.9145\n",
      "Epoch 3/25\n",
      " - 365s - loss: 0.2811 - acc: 0.9094 - val_loss: 0.2616 - val_acc: 0.9145\n",
      "Epoch 4/25\n",
      " - 400s - loss: 0.2686 - acc: 0.9090 - val_loss: 0.2516 - val_acc: 0.9137\n",
      "Epoch 5/25\n",
      " - 380s - loss: 0.2614 - acc: 0.9106 - val_loss: 0.2488 - val_acc: 0.9148\n",
      "Epoch 6/25\n",
      " - 383s - loss: 0.2555 - acc: 0.9088 - val_loss: 0.2445 - val_acc: 0.9148\n",
      "Epoch 7/25\n",
      " - 383s - loss: 0.2482 - acc: 0.9100 - val_loss: 0.2489 - val_acc: 0.9137\n",
      "Epoch 8/25\n",
      " - 370s - loss: 0.2429 - acc: 0.9120 - val_loss: 0.2405 - val_acc: 0.9175\n",
      "Epoch 9/25\n",
      " - 358s - loss: 0.2396 - acc: 0.9115 - val_loss: 0.2377 - val_acc: 0.9171\n",
      "Epoch 10/25\n",
      " - 377s - loss: 0.2370 - acc: 0.9126 - val_loss: 0.2366 - val_acc: 0.9167\n",
      "Epoch 11/25\n",
      " - 358s - loss: 0.2336 - acc: 0.9148 - val_loss: 0.2343 - val_acc: 0.9171\n",
      "Epoch 12/25\n",
      " - 367s - loss: 0.2303 - acc: 0.9132 - val_loss: 0.2328 - val_acc: 0.9186\n",
      "Epoch 13/25\n",
      " - 366s - loss: 0.2282 - acc: 0.9142 - val_loss: 0.2299 - val_acc: 0.9190\n",
      "Epoch 14/25\n",
      " - 364s - loss: 0.2231 - acc: 0.9154 - val_loss: 0.2285 - val_acc: 0.9171\n",
      "Epoch 15/25\n",
      " - 368s - loss: 0.2190 - acc: 0.9169 - val_loss: 0.2274 - val_acc: 0.9148\n",
      "Epoch 16/25\n",
      " - 381s - loss: 0.2194 - acc: 0.9180 - val_loss: 0.2275 - val_acc: 0.9148\n",
      "Epoch 17/25\n",
      " - 372s - loss: 0.2193 - acc: 0.9179 - val_loss: 0.2260 - val_acc: 0.9164\n",
      "Epoch 18/25\n",
      " - 398s - loss: 0.2189 - acc: 0.9186 - val_loss: 0.2252 - val_acc: 0.9171\n",
      "Epoch 19/25\n",
      " - 383s - loss: 0.2129 - acc: 0.9196 - val_loss: 0.2270 - val_acc: 0.9141\n",
      "Epoch 20/25\n",
      " - 374s - loss: 0.2088 - acc: 0.9190 - val_loss: 0.2257 - val_acc: 0.9141\n",
      "Epoch 21/25\n",
      " - 359s - loss: 0.2116 - acc: 0.9187 - val_loss: 0.2261 - val_acc: 0.9133\n",
      "Epoch 22/25\n",
      " - 383s - loss: 0.2079 - acc: 0.9220 - val_loss: 0.2269 - val_acc: 0.9111\n",
      "Epoch 23/25\n",
      " - 377s - loss: 0.2052 - acc: 0.9207 - val_loss: 0.2242 - val_acc: 0.9137\n",
      "Epoch 24/25\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=10, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing...')\n",
    "score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
