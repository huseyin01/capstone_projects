{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "## Regular Expression\n",
    "import re\n",
    "\n",
    "## Arrays\n",
    "import numpy as np\n",
    "\n",
    "## DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "\n",
    "## Modeling\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    12080\n",
      "0     1192\n",
      "Name: rating_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('data_capstone_2/nlp_reviews_cleaned.csv', delimiter=',')\n",
    "\n",
    "df['rating_class'] = df['rating'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "print(df.rating_class.value_counts())\n",
    "\n",
    "df = df.drop(['Unnamed: 0', 'customer', 'product', 'rating', 'time', 'pos_feedback',\n",
    "       'neg_feedback', 'review_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[:10000]\n",
    "df_test = df[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 21393.11it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp_corpus = df_train['clean_text'].map(lambda x: x.split('.'))\n",
    "\n",
    "# corpus [[w1,w2,w3..],[..]]\n",
    "corpus = []\n",
    "for i in tqdm(range(len(tmp_corpus))):\n",
    "    for line in tmp_corpus[i]:\n",
    "        words = [x for x in line.split()]\n",
    "        corpus.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences - 10000\n",
      "Num of words - 802399\n"
     ]
    }
   ],
   "source": [
    "num_of_sentences = len(corpus)\n",
    "num_of_words = 0\n",
    "for line in corpus:\n",
    "    num_of_words += len(line)\n",
    "\n",
    "print('Num of sentences - %s'%(num_of_sentences))\n",
    "print('Num of words - %s'%(num_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg - skip gram |  window = size of the window | size = vector dimension\n",
    "size = 300\n",
    "window_size = 2 # sentences weren't too long, so\n",
    "epochs = 100\n",
    "min_count = 1\n",
    "workers = 4\n",
    "\n",
    "# train word2vec model using gensim\n",
    "model = Word2Vec(corpus, sg=1,window=window_size,size=size,\n",
    "                 min_count=min_count,workers=workers,iter=epochs,sample=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shutofforgilmour', 0.48590296506881714),\n",
       " ('shutoffjust', 0.46847108006477356),\n",
       " ('shutoffor', 0.46435868740081787),\n",
       " ('rem', 0.460856556892395),\n",
       " ('puwebhamz', 0.45755651593208313),\n",
       " ('inchbyfoot', 0.4468499422073364),\n",
       " ('amelnor', 0.44322195649147034),\n",
       " ('apex', 0.4362832009792328),\n",
       " ('uncurl', 0.43391701579093933),\n",
       " ('handgilmour', 0.4314667880535126)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('hose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25974, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary,\n",
    "                                     num_features) for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = df_train['clean_text'].apply(word_tokenize)\n",
    "\n",
    "train_array = averaged_word_vectorizer(corpus=train_corpus, model=model,\n",
    "                                             num_features=300)\n",
    "\n",
    "test_corpus = df_test['clean_text'].apply(word_tokenize)\n",
    "\n",
    "test_array = averaged_word_vectorizer(corpus=test_corpus, model=model,\n",
    "                                             num_features=300)\n",
    "\n",
    "y_test = df_test['rating_class']\n",
    "y_train = df_train['rating_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Accuarcy: 0.9150366748166259\n",
      "\n",
      "2. The F-1 score of the model 0.8818906323979591\n",
      "\n",
      "3. The recall score of the model 0.9150366748166259\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.06      0.11       286\n",
      "           1       0.92      1.00      0.96      2986\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      3272\n",
      "   macro avg       0.78      0.53      0.54      3272\n",
      "weighted avg       0.89      0.92      0.88      3272\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[  18  268]\n",
      " [  10 2976]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=1)\n",
    "clf.fit(train_array, y_train)\n",
    "y_pred_clf = clf.predict(test_array)\n",
    "cm = confusion_matrix(y_test, y_pred_clf)\n",
    "\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_test, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "        classification_report(y_test, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
