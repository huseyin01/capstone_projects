{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "## Regular Expression\n",
    "import re\n",
    "\n",
    "## Arrays\n",
    "import numpy as np\n",
    "\n",
    "## DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "\n",
    "## Modeling\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    12080\n",
      "0     1192\n",
      "Name: rating_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#reading the data\n",
    "df=pd.read_csv('data_capstone_2/nlp_reviews_cleaned.csv', delimiter=',')\n",
    "\n",
    "#creating the classes\n",
    "df['rating_class'] = df['rating'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "print(df.rating_class.value_counts())\n",
    "\n",
    "#train data set reduced due to capacity of computing\n",
    "df_train = df[0:10000]\n",
    "\n",
    "#splitting data set into train and test sets\n",
    "X = df_train['clean_text']\n",
    "y = df_train['rating_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital Scores with best Vectorizer (CountVectorizer) and best n-gram (unigram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word vector with CountVectorizer\n",
    "\n",
    "count_vect1 = CountVectorizer(ngram_range=(1,1))\n",
    "count_vect_train1 = count_vect1.fit_transform(X_train)\n",
    "count_vect_train1 = count_vect_train1.toarray()\n",
    "count_vect_test1 = count_vect1.transform(X_test)\n",
    "count_vect_test1 = count_vect_test1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [Logistic Regression] **********\n",
      "\n",
      "1. Accuarcy: 0.9\n",
      "\n",
      "2. The F-1 score of the model 0.8866628460110704\n",
      "\n",
      "3. The recall score of the model 0.9\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.25      0.32       118\n",
      "           1       0.92      0.97      0.95      1132\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1250\n",
      "   macro avg       0.69      0.61      0.63      1250\n",
      "weighted avg       0.88      0.90      0.89      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[  29   89]\n",
      " [  36 1096]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [Linear SVM] **********\n",
      "\n",
      "1. Accuarcy: 0.8928\n",
      "\n",
      "2. The F-1 score of the model 0.8883063890510475\n",
      "\n",
      "3. The recall score of the model 0.8928\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.35      0.38       118\n",
      "           1       0.93      0.95      0.94      1132\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1250\n",
      "   macro avg       0.68      0.65      0.66      1250\n",
      "weighted avg       0.88      0.89      0.89      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[  41   77]\n",
      " [  57 1075]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [Naive Bayes] **********\n",
      "\n",
      "1. Accuarcy: 0.8048\n",
      "\n",
      "2. The F-1 score of the model 0.8166696388402489\n",
      "\n",
      "3. The recall score of the model 0.8048\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.13      0.11       118\n",
      "           1       0.91      0.88      0.89      1132\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1250\n",
      "   macro avg       0.50      0.50      0.50      1250\n",
      "weighted avg       0.83      0.80      0.82      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[ 15 103]\n",
      " [141 991]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [KNN] **********\n",
      "\n",
      "1. Accuarcy: 0.8824\n",
      "\n",
      "2. The F-1 score of the model 0.8537033028406354\n",
      "\n",
      "3. The recall score of the model 0.8824\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.03      0.05       118\n",
      "           1       0.91      0.97      0.94      1132\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1250\n",
      "   macro avg       0.51      0.50      0.49      1250\n",
      "weighted avg       0.83      0.88      0.85      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[   4  114]\n",
      " [  33 1099]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [Random Forest] **********\n",
      "\n",
      "1. Accuarcy: 0.904\n",
      "\n",
      "2. The F-1 score of the model 0.8695320018672488\n",
      "\n",
      "3. The recall score of the model 0.904\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.06      0.10       118\n",
      "           1       0.91      0.99      0.95      1132\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1250\n",
      "   macro avg       0.67      0.53      0.53      1250\n",
      "weighted avg       0.87      0.90      0.87      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[   7  111]\n",
      " [   9 1123]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [GradientBoosting] **********\n",
      "\n",
      "1. Accuarcy: 0.9048\n",
      "\n",
      "2. The F-1 score of the model 0.873538345841751\n",
      "\n",
      "3. The recall score of the model 0.9048\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.08      0.14       118\n",
      "           1       0.91      0.99      0.95      1132\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1250\n",
      "   macro avg       0.69      0.54      0.55      1250\n",
      "weighted avg       0.87      0.90      0.87      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[  10  108]\n",
      " [  11 1121]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the scores with the best vector (according to step1 and step2)\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = SVC(kernel = 'linear') \n",
    "clf3 = GaussianNB()\n",
    "clf4 = KNeighborsClassifier()\n",
    "clf5 = RandomForestClassifier(random_state=1)\n",
    "clf6 = GradientBoostingClassifier()\n",
    "\n",
    "labels = ['Logistic Regression', 'Linear SVM', 'Naive Bayes', 'KNN', 'Random Forest', 'GradientBoosting', ]\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6], labels):\n",
    "    clf.fit(count_vect_train1, y_train)\n",
    "    y_pred_clf = clf.predict(count_vect_test1)\n",
    "    cm = confusion_matrix(y_test, y_pred_clf)\n",
    "    \n",
    "    print('\\n********** [{}] **********\\n'.format(label))\n",
    "    print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_test, y_pred_clf)))\n",
    "    print('2. The F-1 score of the model {}\\n'.format(f1_score(y_test, y_pred_clf, average='weighted')))\n",
    "    print('3. The recall score of the model {}\\n'.format(recall_score(y_test, y_pred_clf, average='weighted')))\n",
    "    print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "        classification_report(y_test, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Logisric Regression - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Logistic Regression **********\n",
      "\n",
      "Tuned Logistic Regression Parameters: {'C': 0.1, 'penalty': 'l2'}\n",
      "Best score is 0.9189333333333334\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2500, 1250]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-9018467e90c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tuned Logistic Regression Parameters: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best score is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1. Accuarcy: {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2. The F-1 score of the model {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3. The recall score of the model {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2500, 1250]"
     ]
    }
   ],
   "source": [
    "# define the paramater spaces\n",
    "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, params, cv=5)\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(count_vect_train1, y_train)\n",
    "# Print the tuned parameters and score\n",
    "\n",
    "print('\\n********** Logistic Regression **********\\n')\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_test, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "    classification_report(y_test, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Logisric Regression - Modeling with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [Logistic Regression] **********\n",
      "\n",
      "1. Accuarcy: 0.9064\n",
      "\n",
      "2. The F-1 score of the model 0.8817491157291112\n",
      "\n",
      "3. The recall score of the model 0.9064\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.14      0.23       118\n",
      "           1       0.92      0.99      0.95      1132\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1250\n",
      "   macro avg       0.72      0.56      0.59      1250\n",
      "weighted avg       0.88      0.91      0.88      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[  17  101]\n",
      " [  16 1116]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = 'Logistic Regression'\n",
    "clf = LogisticRegression(C=0.1, penalty='l2')\n",
    "\n",
    "clf.fit(count_vect_train1, y_train)\n",
    "y_pred_clf = clf.predict(count_vect_test1)\n",
    "cm = confusion_matrix(y_test, y_pred_clf)\n",
    "    \n",
    "print('\\n********** [{}] **********\\n'.format(label))\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_test, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "    classification_report(y_test, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Logisric Regression - Modeling with Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [Logistic Regression] **********\n",
      "\n",
      "1. Accuarcy: 0.9685333333333334\n",
      "\n",
      "2. The F-1 score of the model 0.9652290559106603\n",
      "\n",
      "3. The recall score of the model 0.9685333333333334\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.63      0.77       319\n",
      "           1       0.97      1.00      0.98      3431\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      3750\n",
      "   macro avg       0.98      0.82      0.88      3750\n",
      "weighted avg       0.97      0.97      0.97      3750\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[ 201  118]\n",
      " [   0 3431]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = 'Logistic Regression'\n",
    "clf = LogisticRegression(C=0.1, penalty='l2')\n",
    "\n",
    "clf.fit(count_vect_train1, y_train)\n",
    "y_pred_clf = clf.predict(count_vect_train1)\n",
    "cm = confusion_matrix(y_train, y_pred_clf)\n",
    "    \n",
    "print('\\n********** [{}] **********\\n'.format(label))\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_train, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_train, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_train, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "    classification_report(y_train, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Logisric Regression - Hyperparameter Tuning Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of Hyperparameter tuning (on Test Set): - 0.05 % "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Linear SVM - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9056\n",
      "Tuned Model Parameters: {'C': 0.001, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Specify the hyperparameter space\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "svm=SVC(kernel='linear')\n",
    "svm_cv = GridSearchCV(svm, param_grid, cv=5)\n",
    "\n",
    "# Fit to the training set\n",
    "svm_cv.fit(count_vect_train1, y_train)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(svm_cv.score(count_vect_test1, y_test)))\n",
    "print(\"Tuned Model Parameters: {}\".format(svm_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Linear SVM (Tuned) - Modeling with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [Logistic Regression] **********\n",
      "\n",
      "1. Accuarcy: 0.9056\n",
      "\n",
      "2. The F-1 score of the model 0.8607382031905961\n",
      "\n",
      "3. The recall score of the model 0.9056\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       118\n",
      "           1       0.91      1.00      0.95      1132\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1250\n",
      "   macro avg       0.45      0.50      0.48      1250\n",
      "weighted avg       0.82      0.91      0.86      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[   0  118]\n",
      " [   0 1132]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = 'Logistic Regression'\n",
    "clf = SVC(kernel='linear', C=0.001, gamma=0.001)\n",
    "\n",
    "clf.fit(count_vect_train1, y_train)\n",
    "y_pred_clf = clf.predict(count_vect_test1)\n",
    "cm = confusion_matrix(y_test, y_pred_clf)\n",
    "    \n",
    "print('\\n********** [{}] **********\\n'.format(label))\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_test, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "    classification_report(y_test, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computations for other models delayed due to limitations of capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onlyone/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:   20.5s remaining:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:   24.4s remaining:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   24.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9149333333333334\n",
      "Tuned Model Parameters: {'nb__alpha': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid,\n",
    "                     verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(count_vect_train1, y_train)  # we can use the full data here but im only using xtrain. \n",
    "\n",
    "print(\"Accuracy: {}\".format(model.score(count_vect_train1, y_train)))\n",
    "print(\"Tuned Model Parameters: {}\".format(model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** MultinomialNB on train set **********\n",
      "\n",
      "1. Accuarcy: 0.9546666666666667\n",
      "\n",
      "2. The F-1 score of the model 0.9480687114894255\n",
      "\n",
      "3. The recall score of the model 0.9546666666666667\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.50      0.65       319\n",
      "           1       0.96      1.00      0.98      3431\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      3750\n",
      "   macro avg       0.95      0.75      0.81      3750\n",
      "weighted avg       0.95      0.95      0.95      3750\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[ 158  161]\n",
      " [   9 3422]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_nb = MultinomialNB(alpha = 10)\n",
    "\n",
    "clf_nb.fit(count_vect_train1, y_train)\n",
    "y_pred_clf = clf.predict(count_vect_train1)\n",
    "cm = confusion_matrix(y_train, y_pred_clf)\n",
    "    \n",
    "print('\\n********** MultinomialNB on train set **********\\n')\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_train, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_train, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_train, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "    classification_report(y_train, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [MultinomialNB] **********\n",
      "\n",
      "1. Accuarcy: 0.9048\n",
      "\n",
      "2. The F-1 score of the model 0.8633327710340627\n",
      "\n",
      "3. The recall score of the model 0.9048\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.02      0.03       118\n",
      "           1       0.91      1.00      0.95      1132\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1250\n",
      "   macro avg       0.65      0.51      0.49      1250\n",
      "weighted avg       0.86      0.90      0.86      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[   2  116]\n",
      " [   3 1129]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_clf = clf.predict(count_vect_test1)\n",
    "cm = confusion_matrix(y_test, y_pred_clf)\n",
    "    \n",
    "print('\\n********** [{}] **********\\n'.format(label))\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_test, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "    classification_report(y_test, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
