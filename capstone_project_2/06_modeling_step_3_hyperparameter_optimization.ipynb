{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "## Regular Expression\n",
    "import re\n",
    "\n",
    "## Arrays\n",
    "import numpy as np\n",
    "\n",
    "## DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "\n",
    "## Modeling\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    12080\n",
      "0     1192\n",
      "Name: rating_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#reading the data\n",
    "df=pd.read_csv('nlp_reviews_cleaned.csv', delimiter=',')\n",
    "\n",
    "#creating the classes\n",
    "df['rating_class'] = df['rating'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "print(df.rating_class.value_counts())\n",
    "\n",
    "#train data set reduced due to capacity of computing\n",
    "df_train = df[0:5000]\n",
    "\n",
    "#splitting data set into train and test sets\n",
    "X = df_train['clean_text']\n",
    "y = df_train['rating_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital Scores with best Vectorizer (CountVectorizer) and best n-gram (unigram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word vector with CountVectorizer\n",
    "\n",
    "count_vect1 = CountVectorizer(ngram_range=(1,1))\n",
    "count_vect_train1 = count_vect1.fit_transform(X_train)\n",
    "count_vect_train1 = count_vect_train1.toarray()\n",
    "count_vect_test1 = count_vect1.transform(X_test)\n",
    "count_vect_test1 = count_vect_test1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [Logistic Regression] **********\n",
      "\n",
      "1. Accuarcy: 0.9\n",
      "\n",
      "2. The F-1 score of the model 0.8866628460110704\n",
      "\n",
      "3. The recall score of the model 0.9\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.25      0.32       118\n",
      "           1       0.92      0.97      0.95      1132\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1250\n",
      "   macro avg       0.69      0.61      0.63      1250\n",
      "weighted avg       0.88      0.90      0.89      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[  29   89]\n",
      " [  36 1096]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [Linear SVM] **********\n",
      "\n",
      "1. Accuarcy: 0.8928\n",
      "\n",
      "2. The F-1 score of the model 0.8883063890510475\n",
      "\n",
      "3. The recall score of the model 0.8928\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.35      0.38       118\n",
      "           1       0.93      0.95      0.94      1132\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1250\n",
      "   macro avg       0.68      0.65      0.66      1250\n",
      "weighted avg       0.88      0.89      0.89      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[  41   77]\n",
      " [  57 1075]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [Naive Bayes] **********\n",
      "\n",
      "1. Accuarcy: 0.8048\n",
      "\n",
      "2. The F-1 score of the model 0.8166696388402489\n",
      "\n",
      "3. The recall score of the model 0.8048\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.13      0.11       118\n",
      "           1       0.91      0.88      0.89      1132\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1250\n",
      "   macro avg       0.50      0.50      0.50      1250\n",
      "weighted avg       0.83      0.80      0.82      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[ 15 103]\n",
      " [141 991]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [KNN] **********\n",
      "\n",
      "1. Accuarcy: 0.8824\n",
      "\n",
      "2. The F-1 score of the model 0.8537033028406354\n",
      "\n",
      "3. The recall score of the model 0.8824\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.03      0.05       118\n",
      "           1       0.91      0.97      0.94      1132\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1250\n",
      "   macro avg       0.51      0.50      0.49      1250\n",
      "weighted avg       0.83      0.88      0.85      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[   4  114]\n",
      " [  33 1099]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [Random Forest] **********\n",
      "\n",
      "1. Accuarcy: 0.904\n",
      "\n",
      "2. The F-1 score of the model 0.8695320018672488\n",
      "\n",
      "3. The recall score of the model 0.904\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.06      0.10       118\n",
      "           1       0.91      0.99      0.95      1132\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1250\n",
      "   macro avg       0.67      0.53      0.53      1250\n",
      "weighted avg       0.87      0.90      0.87      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[   7  111]\n",
      " [   9 1123]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** [GradientBoosting] **********\n",
      "\n",
      "1. Accuarcy: 0.9048\n",
      "\n",
      "2. The F-1 score of the model 0.873538345841751\n",
      "\n",
      "3. The recall score of the model 0.9048\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.08      0.14       118\n",
      "           1       0.91      0.99      0.95      1132\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1250\n",
      "   macro avg       0.69      0.54      0.55      1250\n",
      "weighted avg       0.87      0.90      0.87      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[  10  108]\n",
      " [  11 1121]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the scores with the best vector (according to step1 and step2)\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = SVC(kernel = 'linear') \n",
    "clf3 = GaussianNB()\n",
    "clf4 = KNeighborsClassifier()\n",
    "clf5 = RandomForestClassifier(random_state=1)\n",
    "clf6 = GradientBoostingClassifier()\n",
    "\n",
    "labels = ['Logistic Regression', 'Linear SVM', 'Naive Bayes', 'KNN', 'Random Forest', 'GradientBoosting', ]\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6], labels):\n",
    "    clf.fit(count_vect_train1, y_train)\n",
    "    y_pred_clf = clf.predict(count_vect_test1)\n",
    "    cm = confusion_matrix(y_test, y_pred_clf)\n",
    "    \n",
    "    print('\\n********** [{}] **********\\n'.format(label))\n",
    "    print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_test, y_pred_clf)))\n",
    "    print('2. The F-1 score of the model {}\\n'.format(f1_score(y_test, y_pred_clf, average='weighted')))\n",
    "    print('3. The recall score of the model {}\\n'.format(recall_score(y_test, y_pred_clf, average='weighted')))\n",
    "    print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "        classification_report(y_test, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Logisric Regression - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.1, 'penalty': 'l2'}\n",
      "Best score is 0.9178666666666667\n"
     ]
    }
   ],
   "source": [
    "# define the paramater spaces\n",
    "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, params, cv=5)\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(count_vect_train1, y_train)\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Logisric Regression - Modeling with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [Logistic Regression] **********\n",
      "\n",
      "1. Accuarcy: 0.9064\n",
      "\n",
      "2. The F-1 score of the model 0.8817491157291112\n",
      "\n",
      "3. The recall score of the model 0.9064\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.14      0.23       118\n",
      "           1       0.92      0.99      0.95      1132\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1250\n",
      "   macro avg       0.72      0.56      0.59      1250\n",
      "weighted avg       0.88      0.91      0.88      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[  17  101]\n",
      " [  16 1116]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = 'Logistic Regression'\n",
    "clf = LogisticRegression(C=0.1, penalty='l2')\n",
    "\n",
    "clf.fit(count_vect_train1, y_train)\n",
    "y_pred_clf = clf.predict(count_vect_test1)\n",
    "cm = confusion_matrix(y_test, y_pred_clf)\n",
    "    \n",
    "print('\\n********** [{}] **********\\n'.format(label))\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_test, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "    classification_report(y_test, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Logisric Regression - Modeling with Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [Logistic Regression] **********\n",
      "\n",
      "1. Accuarcy: 0.9685333333333334\n",
      "\n",
      "2. The F-1 score of the model 0.9652290559106603\n",
      "\n",
      "3. The recall score of the model 0.9685333333333334\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.63      0.77       319\n",
      "           1       0.97      1.00      0.98      3431\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      3750\n",
      "   macro avg       0.98      0.82      0.88      3750\n",
      "weighted avg       0.97      0.97      0.97      3750\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[ 201  118]\n",
      " [   0 3431]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = 'Logistic Regression'\n",
    "clf = LogisticRegression(C=0.1, penalty='l2')\n",
    "\n",
    "clf.fit(count_vect_train1, y_train)\n",
    "y_pred_clf = clf.predict(count_vect_train1)\n",
    "cm = confusion_matrix(y_train, y_pred_clf)\n",
    "    \n",
    "print('\\n********** [{}] **********\\n'.format(label))\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_train, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_train, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_train, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "    classification_report(y_train, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Logisric Regression - Hyperparameter Tuning Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of Hyperparameter tuning (on Test Set): - 0.05 % "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Linear SVM - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9056\n",
      "Tuned Model Parameters: {'C': 0.001, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Specify the hyperparameter space\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "svm=SVC(kernel='linear')\n",
    "svm_cv = GridSearchCV(svm, param_grid, cv=5)\n",
    "\n",
    "# Fit to the training set\n",
    "svm_cv.fit(count_vect_train1, y_train)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(svm_cv.score(count_vect_test1, y_test)))\n",
    "print(\"Tuned Model Parameters: {}\".format(svm_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Linear SVM - Modeling with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** [Logistic Regression] **********\n",
      "\n",
      "1. Accuarcy: 0.9056\n",
      "\n",
      "2. The F-1 score of the model 0.8607382031905961\n",
      "\n",
      "3. The recall score of the model 0.9056\n",
      "\n",
      "4. Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       118\n",
      "           1       0.91      1.00      0.95      1132\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1250\n",
      "   macro avg       0.45      0.50      0.48      1250\n",
      "weighted avg       0.82      0.91      0.86      1250\n",
      "\n",
      "5. Confusion matrix:\n",
      "[[   0  118]\n",
      " [   0 1132]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = 'Logistic Regression'\n",
    "clf = SVC(kernel='linear', C=0.001, gamma=0.001)\n",
    "\n",
    "clf.fit(count_vect_train1, y_train)\n",
    "y_pred_clf = clf.predict(count_vect_test1)\n",
    "cm = confusion_matrix(y_test, y_pred_clf)\n",
    "    \n",
    "print('\\n********** [{}] **********\\n'.format(label))\n",
    "print('1. Accuarcy: {}\\n'.format(metrics.accuracy_score(y_test, y_pred_clf)))\n",
    "print('2. The F-1 score of the model {}\\n'.format(f1_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('3. The recall score of the model {}\\n'.format(recall_score(y_test, y_pred_clf, average='weighted')))\n",
    "print('4. Classification Report:\\n{}\\n5. Confusion matrix:\\n{}\\n\\n\\n'.format(\n",
    "    classification_report(y_test, y_pred_clf), cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computations for other models delayed due to limitations of capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
