{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Keras with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    12080\n",
      "0     1192\n",
      "Name: rating, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great Hoses Good USA company that stands behin...</td>\n",
       "      <td>great hose good usa company stand behind produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gilmour 10-58050 8-ply Flexogen Hose 5/8-Inch ...</td>\n",
       "      <td>gilmour ply flexogen hose inch foot green high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Very satisfied! It's probably one of the best ...</td>\n",
       "      <td>satisfied probably one best hose ever pro good...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                        review_text  \\\n",
       "0       1  Great Hoses Good USA company that stands behin...   \n",
       "1       1  Gilmour 10-58050 8-ply Flexogen Hose 5/8-Inch ...   \n",
       "2       1  Very satisfied! It's probably one of the best ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  great hose good usa company stand behind produ...  \n",
       "1  gilmour ply flexogen hose inch foot green high...  \n",
       "2  satisfied probably one best hose ever pro good...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('data_capstone_2/nlp_reviews_cleaned.csv', delimiter=',', encoding='utf-8')\n",
    "\n",
    "df['rating'] = df['rating'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "print(df.rating.value_counts())\n",
    "\n",
    "df = df.drop(['Unnamed: 0', 'customer', 'product', 'time', 'pos_feedback',\n",
    "       'neg_feedback'], axis=1)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "review_lines = list()\n",
    "lines = df['review_text'].values.tolist()\n",
    "\n",
    "for line in lines:   \n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13272"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 44864\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "# vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "filename = 'patio_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vegetable', 0.8768383264541626),\n",
       " ('beds', 0.8337284326553345),\n",
       " ('bed', 0.8252702951431274),\n",
       " ('gardens', 0.8229961395263672),\n",
       " ('gardenwell', 0.8200172185897827),\n",
       " ('tilled', 0.812398374080658),\n",
       " ('flower', 0.8111097812652588),\n",
       " ('convenientmost', 0.8109019994735718),\n",
       " ('potted', 0.8022102117538452),\n",
       " ('global', 0.8003907799720764)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us try some utility functions of gensim word2vec more details here \n",
    "\n",
    "model.wv.most_similar('garden')#, topn =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trees', 1.2877529859542847),\n",
       " ('asspectracide', 1.2751572132110596),\n",
       " ('neardesert', 1.2589832544326782),\n",
       " ('sharon', 1.2445369958877563),\n",
       " ('bushes', 1.2419459819793701),\n",
       " ('weeds', 1.2363262176513672),\n",
       " ('graveled', 1.2342287302017212),\n",
       " ('gardens', 1.2331007719039917),\n",
       " ('shrubs', 1.2270678281784058),\n",
       " ('hemlock', 1.222316861152649)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s see the result of semantically reasonable word vectors (king - man + woman)\n",
    "model.wv.most_similar_cosmul(positive=['garden', 'lawn'], negative=['hose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bushes', 0.7804819941520691),\n",
       " ('gardens', 0.7714396715164185),\n",
       " ('shrubs', 0.768234133720398),\n",
       " ('weeds', 0.7635079622268677),\n",
       " ('trees', 0.7633467316627502),\n",
       " ('weed', 0.7562059760093689),\n",
       " ('growing', 0.7357085943222046),\n",
       " ('asspectracide', 0.7348328828811646),\n",
       " ('lowhanging', 0.7326929569244385),\n",
       " ('greening', 0.7302519679069519)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s see the result of semantically reasonable word vectors (king - man + woman)\n",
    "model.wv.most_similar(positive=['garden', 'lawn'], negative=['hose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hose\n"
     ]
    }
   ],
   "source": [
    "#odd word out\n",
    "print(model.wv.doesnt_match(\"garden king lawn hose\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vegetable', 0.8768383264541626),\n",
       " ('beds', 0.8337284326553345),\n",
       " ('bed', 0.8252702951431274),\n",
       " ('gardens', 0.8229961395263672),\n",
       " ('gardenwell', 0.8200172185897827),\n",
       " ('tilled', 0.812398374080658),\n",
       " ('flower', 0.8111097812652588),\n",
       " ('convenientmost', 0.8109019994735718),\n",
       " ('potted', 0.8022102117538452),\n",
       " ('global', 0.8003907799720764)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"garden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5919059\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('garden', 'lawn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'patio_embedding_word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:10000, 'review_text'].values\n",
    "y_train = df.loc[:10000, 'rating'].values\n",
    "X_test = df.loc[10001:, 'review_text'].values\n",
    "y_test = df.loc[10001:, 'rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = df.review_text.values\n",
    "max_length = max([len(s.split()) for s in total_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44864 unique tokens.\n",
      "Shape of review tensor: (13272, 2263)\n",
      "Shape of sentiment tensor: (13272,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(review_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment =  df['rating'].values\n",
    "print('Shape of review tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor:', sentiment.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad tensor: (10618, 2263)\n",
      "Shape of y_train tensor: (10618,)\n",
      "Shape of X_test_pad tensor: (2654, 2263)\n",
      "Shape of y_test tensor: (2654,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad tensor:', X_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44865\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2263, 100)         4486500   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2259, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1129, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 144512)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 144513    \n",
      "=================================================================\n",
      "Total params: 4,695,141\n",
      "Trainable params: 208,641\n",
      "Non-trainable params: 4,486,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10618 samples, validate on 2654 samples\n",
      "Epoch 1/10\n",
      " - 263s - loss: 0.3001 - acc: 0.9023 - val_loss: 0.2724 - val_acc: 0.9126\n",
      "Epoch 2/10\n",
      " - 214s - loss: 0.2447 - acc: 0.9107 - val_loss: 0.2685 - val_acc: 0.9077\n",
      "Epoch 3/10\n",
      " - 219s - loss: 0.2196 - acc: 0.9166 - val_loss: 0.2682 - val_acc: 0.9115\n",
      "Epoch 4/10\n",
      " - 215s - loss: 0.1876 - acc: 0.9258 - val_loss: 0.2741 - val_acc: 0.9066\n",
      "Epoch 5/10\n",
      " - 199s - loss: 0.1538 - acc: 0.9391 - val_loss: 0.2905 - val_acc: 0.9077\n",
      "Epoch 6/10\n",
      " - 207s - loss: 0.1232 - acc: 0.9528 - val_loss: 0.3206 - val_acc: 0.9069\n",
      "Epoch 7/10\n",
      " - 197s - loss: 0.0938 - acc: 0.9666 - val_loss: 0.3434 - val_acc: 0.9043\n",
      "Epoch 8/10\n",
      " - 200s - loss: 0.0703 - acc: 0.9782 - val_loss: 0.3901 - val_acc: 0.9069\n",
      "Epoch 9/10\n",
      " - 200s - loss: 0.0530 - acc: 0.9860 - val_loss: 0.4231 - val_acc: 0.9073\n",
      "Epoch 10/10\n",
      " - 203s - loss: 0.0426 - acc: 0.9884 - val_loss: 0.4094 - val_acc: 0.8945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x117ddf358>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=10, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2654/2654 [==============================] - 21s 8ms/step\n",
      "Accuracy: 89.449887\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9321882 ],\n",
       "       [0.9743523 ],\n",
       "       [0.4674715 ],\n",
       "       [0.9235527 ],\n",
       "       [0.69168115],\n",
       "       [0.62217313],\n",
       "       [0.9743523 ],\n",
       "       [0.82867193]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us test some  samples\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "\n",
    "test_sample_1 = \"This garden is beautiful!\"\n",
    "test_sample_2 = \"Good hose!\"\n",
    "test_sample_3 = \"Maybe I like this lawnmover.\"\n",
    "test_sample_4 = \"Not to my taste, will skip and use another hose\"\n",
    "test_sample_5 = \"if you like fruits, then this product might be good for you.\"\n",
    "test_sample_6 = \"Bad rubber!\"\n",
    "test_sample_7 = \"Not a good vegetable!\"\n",
    "test_sample_8 = \"This hose really sucks! Can I get my money back please?\"\n",
    "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]\n",
    "\n",
    "test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
    "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=2263)\n",
    "\n",
    "#predict\n",
    "model.predict(x=test_samples_tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the built model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 2263, 100)         4486500   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,499,301\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 4,486,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 10618 samples, validate on 2654 samples\n",
      "Epoch 1/10\n",
      " - 276s - loss: 0.3230 - acc: 0.9033 - val_loss: 0.2871 - val_acc: 0.9130\n",
      "Epoch 2/10\n",
      " - 271s - loss: 0.2884 - acc: 0.9100 - val_loss: 0.2733 - val_acc: 0.9130\n",
      "Epoch 3/10\n",
      " - 308s - loss: 0.2734 - acc: 0.9098 - val_loss: 0.2601 - val_acc: 0.9133\n",
      "Epoch 4/10\n",
      " - 289s - loss: 0.2585 - acc: 0.9094 - val_loss: 0.2563 - val_acc: 0.9145\n",
      "Epoch 5/10\n",
      " - 281s - loss: 0.2478 - acc: 0.9097 - val_loss: 0.2548 - val_acc: 0.9148\n",
      "Epoch 6/10\n",
      " - 281s - loss: 0.2435 - acc: 0.9103 - val_loss: 0.2466 - val_acc: 0.9160\n",
      "Epoch 7/10\n",
      " - 280s - loss: 0.2411 - acc: 0.9121 - val_loss: 0.2447 - val_acc: 0.9167\n",
      "Epoch 8/10\n",
      " - 274s - loss: 0.2370 - acc: 0.9130 - val_loss: 0.2426 - val_acc: 0.9160\n",
      "Epoch 9/10\n",
      " - 280s - loss: 0.2309 - acc: 0.9123 - val_loss: 0.2421 - val_acc: 0.9164\n",
      "Epoch 10/10\n",
      " - 294s - loss: 0.2302 - acc: 0.9116 - val_loss: 0.2420 - val_acc: 0.9175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a438786a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=10, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "2654/2654 [==============================] - 17s 6ms/step\n",
      "Test score: 0.24200316827909876\n",
      "Test accuracy: 0.9174830450001927\n",
      "Accuracy: 91.75%\n"
     ]
    }
   ],
   "source": [
    "print('Testing...')\n",
    "score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.07      0.13       231\n",
      "           1       0.92      1.00      0.96      2423\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2654\n",
      "   macro avg       0.85      0.54      0.55      2654\n",
      "weighted avg       0.91      0.92      0.89      2654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test_pad)\n",
    "print (classification_report(y_test, y_pred.round()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
