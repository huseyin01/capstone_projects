{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Best N-Gram for Vectorizing (without tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arrays\n",
    "import numpy as np\n",
    "\n",
    "## DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "## Visualizations\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "## Modeling\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    12080\n",
      "0     1192\n",
      "Name: rating_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#reading the data\n",
    "df=pd.read_csv('nlp_reviews_cleaned.csv', delimiter=',')\n",
    "\n",
    "#creating the classes\n",
    "df['rating_class'] = df['rating'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "print(df.rating_class.value_counts())\n",
    "\n",
    "#train data set reduced due to capacity of computing\n",
    "df_train = df[0:5000]\n",
    "\n",
    "#splitting data set into train and test sets\n",
    "X = df_train['clean_text']\n",
    "y = df_train['rating_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finding the Best N-Gram for CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating four different CountVectors for each n-gram\n",
    "\n",
    "count_vect1 = CountVectorizer(ngram_range=(1,1))\n",
    "count_vect_train1 = count_vect1.fit_transform(X_train)\n",
    "count_vect_train1 = count_vect_train1.toarray()\n",
    "count_vect_test1 = count_vect1.transform(X_test)\n",
    "count_vect_test1 = count_vect_test1.toarray()\n",
    "\n",
    "count_vect2 = CountVectorizer(ngram_range=(2,2))\n",
    "count_vect_train2 = count_vect2.fit_transform(X_train)\n",
    "count_vect_train2 = count_vect_train2.toarray()\n",
    "count_vect_test2 = count_vect2.transform(X_test)\n",
    "count_vect_test2 = count_vect_test2.toarray()\n",
    "\n",
    "count_vect3 = CountVectorizer(ngram_range=(3,3))\n",
    "count_vect_train3 = count_vect3.fit_transform(X_train)\n",
    "count_vect_train3 = count_vect_train3.toarray()\n",
    "count_vect_test3 = count_vect3.transform(X_test)\n",
    "count_vect_test3 = count_vect_test3.toarray()\n",
    "\n",
    "count_vect4 = CountVectorizer(ngram_range=(4,4))\n",
    "count_vect_train4 = count_vect4.fit_transform(X_train)\n",
    "count_vect_train4 = count_vect_train4.toarray()\n",
    "count_vect_test4 = count_vect4.transform(X_test)\n",
    "count_vect_test4 = count_vect_test4.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CountVectorizer N-grams with Logistic regression\n",
      "\n",
      "Accuracy : 0.9 | F-1 Score: 0.8866628460110704 '[Uni-gram]'\n",
      "Accuracy : 0.9056 | F-1 Score: 0.8651926577603998 '[Bi-gram]'\n",
      "Accuracy : 0.9056 | F-1 Score: 0.862273837535014 '[Tri-gram]'\n",
      "Accuracy : 0.9064 | F-1 Score: 0.8626862606277287 '[Four-gram]'\n"
     ]
    }
   ],
   "source": [
    "# Trying each vector with the same model\n",
    "# only one algoritm was used due to computing capacity\n",
    "# LogisticRegression was chosen as the best model at step1\n",
    "\n",
    "logreg_CV = LogisticRegression()\n",
    "\n",
    "print('\\nCountVectorizer N-grams with Logistic regression\\n')\n",
    "\n",
    "labels = ['Uni-gram', 'Bi-gram', 'Tri-gram', 'Four-gram']\n",
    "\n",
    "for cv_train, cv_test, label in zip([count_vect_train1, count_vect_train2, count_vect_train3, count_vect_train4], \n",
    "                           [count_vect_test1, count_vect_test2, count_vect_test3, count_vect_test4], labels):\n",
    "    \n",
    "    logreg_CV.fit(cv_train, y_train)\n",
    "    y_pred_lr_CV = logreg_CV.predict(cv_test)\n",
    "    print(\"Accuracy : {} | F-1 Score: {} '[{}]'\".format(\n",
    "        metrics.accuracy_score(y_test, y_pred_lr_CV), f1_score(y_test, y_pred_lr_CV, average='weighted'), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Best N-gram for CountVectorizer is Uni-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding the Best N-Gram for HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating four different TfIdf-Vectors for each n-gram\n",
    "\n",
    "tdidf_vect1 = TfidfVectorizer(ngram_range=(1,1))\n",
    "tdidf_vect_train1 = tdidf_vect1.fit_transform(X_train)\n",
    "tdidf_vect_train1 = tdidf_vect_train1.toarray()\n",
    "tdidf_vect_test1 = tdidf_vect1.transform(X_test)\n",
    "tdidf_vect_test1 = tdidf_vect_test1.toarray()\n",
    "\n",
    "tdidf_vect2 = TfidfVectorizer(ngram_range=(2,2))\n",
    "tdidf_vect_train2 = tdidf_vect2.fit_transform(X_train)\n",
    "tdidf_vect_train2 = tdidf_vect_train2.toarray()\n",
    "tdidf_vect_test2 = tdidf_vect2.transform(X_test)\n",
    "tdidf_vect_test2 = tdidf_vect_test2.toarray()\n",
    "\n",
    "tdidf_vect3 = TfidfVectorizer(ngram_range=(3,3))\n",
    "tdidf_vect_train3 = tdidf_vect3.fit_transform(X_train)\n",
    "tdidf_vect_train3 = tdidf_vect_train3.toarray()\n",
    "tdidf_vect_test3 = tdidf_vect3.transform(X_test)\n",
    "tdidf_vect_test3 = tdidf_vect_test3.toarray()\n",
    "\n",
    "tdidf_vect4 = TfidfVectorizer(ngram_range=(4,4))\n",
    "tdidf_vect_train4 = tdidf_vect4.fit_transform(X_train)\n",
    "tdidf_vect_train4 = tdidf_vect_train4.toarray()\n",
    "tdidf_vect_test4 = tdidf_vect4.transform(X_test)\n",
    "tdidf_vect_test4 = tdidf_vect_test4.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Td-idf N-grams with Logistic regression\n",
      "\n",
      "Accuracy : 0.9064 | F-1 Score: 0.8626862606277287 '[Uni-gram]'\n",
      "Accuracy : 0.9056 | F-1 Score: 0.8607382031905961 '[Bi-gram]'\n",
      "Accuracy : 0.9056 | F-1 Score: 0.8607382031905961 '[Tri-gram]'\n",
      "Accuracy : 0.9056 | F-1 Score: 0.8607382031905961 '[Four-gram]'\n"
     ]
    }
   ],
   "source": [
    "# Trying each vector with the same model\n",
    "\n",
    "logreg_CV = LogisticRegression()\n",
    "\n",
    "print('\\nTd-idf N-grams with Logistic regression\\n')\n",
    "\n",
    "labels = ['Uni-gram', 'Bi-gram', 'Tri-gram', 'Four-gram']\n",
    "\n",
    "for tfv_train, tfv_test, label in zip([tdidf_vect_train1, tdidf_vect_train2, tdidf_vect_train3, tdidf_vect_train4], \n",
    "                           [tdidf_vect_test1, tdidf_vect_test2, tdidf_vect_test3, tdidf_vect_test4], labels):\n",
    "    \n",
    "    logreg_CV.fit(tfv_train, y_train)\n",
    "    y_pred_lr_CV = logreg_CV.predict(tfv_test)\n",
    "    print(\"Accuracy : {} | F-1 Score: {} '[{}]'\".format(\n",
    "        metrics.accuracy_score(y_test, y_pred_lr_CV), f1_score(y_test, y_pred_lr_CV, average='weighted'), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Best N-gram for TfIdfVectorizer is Uni-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding the Best N-Gram for TfIdf-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating four different TfIdf-Vectors for each n-gram\n",
    "hash_vect1 = HashingVectorizer(n_features=2000, ngram_range=(1,1))\n",
    "hash_vect_train1 = hash_vect1.fit_transform(X_train)\n",
    "hash_vect_train1 = hash_vect_train1.toarray()\n",
    "hash_vect_test1 = hash_vect1.transform(X_test)\n",
    "hash_vect_test1 = hash_vect_test1.toarray()\n",
    "\n",
    "hash_vect2 = HashingVectorizer(n_features=2000, ngram_range=(2,2))\n",
    "hash_vect_train2 = hash_vect2.fit_transform(X_train)\n",
    "hash_vect_train2 = hash_vect_train2.toarray()\n",
    "hash_vect_test2 = hash_vect1.transform(X_test)\n",
    "hash_vect_test2 = hash_vect_test2.toarray()\n",
    "\n",
    "hash_vect3 = HashingVectorizer(n_features=2000, ngram_range=(3,3))\n",
    "hash_vect_train3 = hash_vect3.fit_transform(X_train)\n",
    "hash_vect_train3 = hash_vect_train3.toarray()\n",
    "hash_vect_test3 = hash_vect3.transform(X_test)\n",
    "hash_vect_test3 = hash_vect_test3.toarray()\n",
    "\n",
    "hash_vect4 = HashingVectorizer(n_features=2000, ngram_range=(4,4))\n",
    "hash_vect_train4 = hash_vect4.fit_transform(X_train)\n",
    "hash_vect_train4 = hash_vect_train4.toarray()\n",
    "hash_vect_test4 = hash_vect4.transform(X_test)\n",
    "hash_vect_test4 = hash_vect_test4.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HashVectorizer N-grams with Logistic regression\n",
      "\n",
      "Accuracy : 0.9056 | F-1 Score: 0.862273837535014 '[Uni-gram]'\n",
      "Accuracy : 0.9056 | F-1 Score: 0.8607382031905961 '[Bi-gram]'\n",
      "Accuracy : 0.9056 | F-1 Score: 0.8607382031905961 '[Tri-gram]'\n",
      "Accuracy : 0.9056 | F-1 Score: 0.8607382031905961 '[Four-gram]'\n"
     ]
    }
   ],
   "source": [
    "# Trying each vector with the same model\n",
    "\n",
    "logreg_CV = LogisticRegression()\n",
    "\n",
    "print('\\nHashVectorizer N-grams with Logistic regression\\n')\n",
    "\n",
    "labels = ['Uni-gram', 'Bi-gram', 'Tri-gram', 'Four-gram']\n",
    "\n",
    "for hv_train, hv_test, label in zip([hash_vect_train1, hash_vect_train2, hash_vect_train3, hash_vect_train4], \n",
    "                           [hash_vect_test1, hash_vect_test2, hash_vect_test3, hash_vect_test4], labels):\n",
    "    \n",
    "    logreg_CV.fit(hv_train, y_train)\n",
    "    y_pred_lr_CV = logreg_CV.predict(hv_test)\n",
    "    print(\"Accuracy : {} | F-1 Score: {} '[{}]'\".format(\n",
    "        metrics.accuracy_score(y_test, y_pred_lr_CV), f1_score(y_test, y_pred_lr_CV, average='weighted'), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: Best N-gram for HashVectorizer is Uni-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "\n",
    "## The best n-gram for all Vectorizers:\n",
    "\n",
    "## Uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
